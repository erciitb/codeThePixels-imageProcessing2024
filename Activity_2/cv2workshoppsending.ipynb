{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACIAL EMOTION RECOGNITION\n",
    "\n",
    "This model detects and classifies human emotions from facial expressions in both static images and live webcam feeds.\n",
    "\n",
    "Libraries used: \n",
    "NumPy\n",
    "OpenCV\n",
    "Matplotlib\n",
    "Tensorflow\n",
    "\n",
    "Authors: ERC, IIT Bombay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "\n",
    "train_data_dir = 'train'\n",
    "val_data_dir = 'test'\n",
    "\n",
    "#printing some data\n",
    "path=os.path.join(train_data_dir,\"happy\")\n",
    "i=0\n",
    "\n",
    "for img in os.listdir(path):\n",
    "    i=i+1\n",
    "    img_array=cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n",
    "    if (i<=3) :\n",
    "        plt.imshow(img_array, cmap=\"gray\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "#data preprocessing, augumenting the images to increase generalization of the model\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, zoom_range=0.2, horizontal_flip=True)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(48, 48), color_mode='grayscale', batch_size=64, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory(val_data_dir, target_size=(48, 48), color_mode='grayscale', batch_size=32, class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_generator, validation_data=val_generator, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = [ 'Happy', 'Sad', 'Surprise']\n",
    "\n",
    "#capturing videos from webcam\n",
    "cap = cv2.VideoCapture(0)   \n",
    "\n",
    "while True:\n",
    "    \n",
    "    #capturing a frame from the live webcam feed\n",
    "    ret, frame = cap.read() \n",
    "\n",
    "    #ret=False means that there is some problem with capturing the frame\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "     # using haar cascade inbuilt model to detect faces\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray_frame[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
    "        roi_gray = roi_gray.astype('float') / 255.0\n",
    "        roi_gray = img_to_array(roi_gray)\n",
    "        roi_gray = np.expand_dims(roi_gray, axis=0)\n",
    "        \n",
    "        emotion_prediction = model.predict(roi_gray)\n",
    "        max_index = np.argmax(emotion_prediction)\n",
    "        emotion = emotion_labels[max_index]\n",
    "        print(emotion)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 4)\n",
    "    \n",
    "    \n",
    "    cv2.imshow('Emoji Emotion Recognition', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):   #press q to close the window\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
